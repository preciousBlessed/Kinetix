# Inherit all PPO defaults but with GRPO-specific parameters
lr: 5e-5
peak_lr: 3e-4
initial_lr: 1e-5
warmup_frac: 0.1
max_grad_norm: 1.0
total_timesteps: 1073741824
num_train_envs: 2048
num_minibatches: 32
gamma: 0.995
update_epochs: 8
clip_eps: 0.2  # Can be slightly reduced for GRPO (e.g., 0.15)
gae_lambda: 0.9
ent_coef: 0.01
anneal_lr: false
warmup_lr: false
vf_coef: 0.5
permute_state_during_training: false
filter_levels: true
level_filter_n_steps: 64
level_filter_sample_ratio: 2

# GRPO-specific parameters
# group_size: 64  # Size of advantage normalization groups
# group_norm_epsilon: 1e-8  # Small constant for numerical stability

# grpo-base.yaml
group_size: 128  # Balanced choice
group_norm_epsilon: 1e-6
use_mixed_precision: true  # If using GPU